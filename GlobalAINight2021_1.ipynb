{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Importing packages here. These packages are commonly used in machine learning projects. We are also printing our Azure ML SDK version number.\r\n",
        "#Explain `as` in `import` statements\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import azureml.core\r\n",
        "from azureml.core import Workspace\r\n",
        "\r\n",
        "# check core SDK version number\r\n",
        "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading our workspace configuration from the config.json. We will load the data from config.json into our object called ws, then print out specific data from it.\r\n",
        "\r\n",
        "ws = Workspace.from_config()\r\n",
        "print(ws.name, ws.location, ws.resource_group, sep='\\t')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating expiriment. Expiriments can be used to track runs throughout the workspace.\r\n",
        "\r\n",
        "experiment_name = 'Global-AI-Night-Image-Classification'\r\n",
        "\r\n",
        "from azureml.core import Experiment\r\n",
        "exp = Experiment(workspace=ws, name=experiment_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We are naming a compute cluster, choosing to use a CPU or GPU VM, checking if it already exists, and then creating the cluster\r\n",
        "#If the compute cluster does not exist already, we create it in the `else` statement\r\n",
        "\r\n",
        "from azureml.core.compute import AmlCompute\r\n",
        "from azureml.core.compute import ComputeTarget\r\n",
        "import os\r\n",
        "\r\n",
        "#choose a name for your cluster\r\n",
        "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpu-cluster\")\r\n",
        "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\r\n",
        "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\r\n",
        "\r\n",
        "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\r\n",
        "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\r\n",
        "\r\n",
        "\r\n",
        "if compute_name in ws.compute_targets:\r\n",
        "    compute_target = ws.compute_targets[compute_name]\r\n",
        "    if compute_target and type(compute_target) is AmlCompute:\r\n",
        "        print(\"found compute target: \" + compute_name)\r\n",
        "else:\r\n",
        "    print(\"creating new compute target...\")\r\n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\r\n",
        "                                                                min_nodes = compute_min_nodes, \r\n",
        "                                                                max_nodes = compute_max_nodes)\r\n",
        "\r\n",
        "    # create the cluster\r\n",
        "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\r\n",
        "    \r\n",
        "    # can poll for a minimum number of nodes and for a specific timeout. \r\n",
        "    # if no min node count is provided it will use the scale settings for the cluster\r\n",
        "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\r\n",
        "    \r\n",
        "     # For a more detailed view of current AmlCompute status, use get_status()\r\n",
        "    print(compute_target.get_status().serialize())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we are downloading the MNIST dataset from Azure Open Datasets. These are public datasets provided by Azure to use.\r\n",
        "#We store the dataset in a FileDataset object from azureml.core's `Dataset` to be used later\r\n",
        "\r\n",
        "from azureml.core import Dataset\r\n",
        "from azureml.opendatasets import MNIST\r\n",
        "\r\n",
        "data_folder = os.path.join(os.getcwd(), 'data')\r\n",
        "os.makedirs(data_folder, exist_ok=True)\r\n",
        "\r\n",
        "mnist_file_dataset = MNIST.get_file_dataset()\r\n",
        "mnist_file_dataset.download(data_folder, overwrite=True)\r\n",
        "\r\n",
        "mnist_file_dataset = mnist_file_dataset.register(workspace=ws,\r\n",
        "                                                 name='mnist_opendataset',\r\n",
        "                                                 description='training and test dataset',\r\n",
        "                                                 create_new_version=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We are displaying some of the images from the MNIST dataset by loading the files into an Numpy array and then using matplotlib to plot 30 images.\r\n",
        "#The utils.py file is essential because we are using the `load_data` function from it. All it's doing is parsing the compressed files we loaded into a Numpy array\r\n",
        "\r\n",
        "# make sure utils.py is in the same directory as this code\r\n",
        "from utils import load_data\r\n",
        "import glob\r\n",
        "\r\n",
        "\r\n",
        "# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the model converge faster.\r\n",
        "X_train = load_data(glob.glob(os.path.join(data_folder,\"**/train-images-idx3-ubyte.gz\"), recursive=True)[0], False) / 255.0\r\n",
        "X_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-images-idx3-ubyte.gz\"), recursive=True)[0], False) / 255.0\r\n",
        "y_train = load_data(glob.glob(os.path.join(data_folder,\"**/train-labels-idx1-ubyte.gz\"), recursive=True)[0], True).reshape(-1)\r\n",
        "y_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-labels-idx1-ubyte.gz\"), recursive=True)[0], True).reshape(-1)\r\n",
        "\r\n",
        "# now let's show some randomly chosen images from the traininng set.\r\n",
        "count = 0\r\n",
        "sample_size = 30\r\n",
        "plt.figure(figsize = (16, 6))\r\n",
        "for i in np.random.permutation(X_train.shape[0])[:sample_size]:\r\n",
        "    count = count + 1\r\n",
        "    plt.subplot(1, sample_size, count)\r\n",
        "    plt.axhline('')\r\n",
        "    plt.axvline('')\r\n",
        "    plt.text(x=10, y=-10, s=y_train[i], fontsize=18)\r\n",
        "    plt.imshow(X_train[i].reshape(28, 28), cmap=plt.cm.Greys)\r\n",
        "plt.show()\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now, we are going to begin training our model on the remote cluster we created earlier.\r\n",
        "#Create directory to transfer the source code to a remote source\r\n",
        "\r\n",
        "import os\r\n",
        "script_folder = os.path.join(os.getcwd(), \"sklearn-mnist\")\r\n",
        "os.makedirs(script_folder, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $script_folder/train.py\r\n",
        "\r\n",
        "#We need to create a script to train our model. When we run this script, we need to provide arguments to decide where are output directory will be.\r\n",
        "\r\n",
        "import argparse\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import glob\r\n",
        "\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "import joblib\r\n",
        "\r\n",
        "from azureml.core import Run\r\n",
        "from utils import load_data\r\n",
        "\r\n",
        "# let user feed in 2 parameters, the dataset to mount or download, and the regularization rate of the logistic regression model\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\r\n",
        "parser.add_argument('--regularization', type=float, dest='reg', default=0.01, help='regularization rate')\r\n",
        "args = parser.parse_args()\r\n",
        "\r\n",
        "data_folder = args.data_folder\r\n",
        "print('Data folder:', data_folder)\r\n",
        "\r\n",
        "# load train and test set into numpy arrays\r\n",
        "# note we scale the pixel intensity values to 0-1 (by dividing it with 255.0) so the model can converge faster.\r\n",
        "X_train = load_data(glob.glob(os.path.join(data_folder, '**/train-images-idx3-ubyte.gz'), recursive=True)[0], False) / 255.0\r\n",
        "X_test = load_data(glob.glob(os.path.join(data_folder, '**/t10k-images-idx3-ubyte.gz'), recursive=True)[0], False) / 255.0\r\n",
        "y_train = load_data(glob.glob(os.path.join(data_folder, '**/train-labels-idx1-ubyte.gz'), recursive=True)[0], True).reshape(-1)\r\n",
        "y_test = load_data(glob.glob(os.path.join(data_folder, '**/t10k-labels-idx1-ubyte.gz'), recursive=True)[0], True).reshape(-1)\r\n",
        "\r\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, sep = '\\n')\r\n",
        "\r\n",
        "# get hold of the current run\r\n",
        "run = Run.get_context()\r\n",
        "\r\n",
        "print('Train a logistic regression model with regularization rate of', args.reg)\r\n",
        "clf = LogisticRegression(C=1.0/args.reg, solver=\"liblinear\", multi_class=\"auto\", random_state=42)\r\n",
        "clf.fit(X_train, y_train)\r\n",
        "\r\n",
        "print('Predict the test set')\r\n",
        "y_hat = clf.predict(X_test)\r\n",
        "\r\n",
        "# calculate accuracy on the prediction\r\n",
        "acc = np.average(y_hat == y_test)\r\n",
        "print('Accuracy is', acc)\r\n",
        "\r\n",
        "run.log('regularization rate', np.float(args.reg))\r\n",
        "run.log('accuracy', np.float(acc))\r\n",
        "\r\n",
        "os.makedirs('outputs', exist_ok=True)\r\n",
        "# note file saved in the outputs folder is automatically uploaded into experiment record\r\n",
        "joblib.dump(value=clf, filename='outputs/sklearn_mnist_model.pkl')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We need this to load our dataset properly\r\n",
        "\r\n",
        "import shutil\r\n",
        "shutil.copy('utils.py', script_folder)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We are specifying the details of our training job. Some of these details include: training script, environment to use, and the compute target to run on. \r\n",
        "#First, we set up our environment with pandas, scikitlearn and more.\r\n",
        "#Next, we create the config by specifying the training script, compute target and environment.\r\n",
        "\r\n",
        "from azureml.core.environment import Environment\r\n",
        "from azureml.core.conda_dependencies import CondaDependencies\r\n",
        "\r\n",
        "# to install required packages\r\n",
        "env = Environment('tutorial-env')\r\n",
        "cd = CondaDependencies.create(pip_packages=['azureml-dataset-runtime[pandas,fuse]', 'azureml-defaults'], conda_packages = ['scikit-learn==0.22.1'])\r\n",
        "\r\n",
        "env.python.conda_dependencies = cd\r\n",
        "\r\n",
        "# Register environment to re-use later\r\n",
        "env.register(workspace = ws)\r\n",
        "\r\n",
        "from azureml.core import ScriptRunConfig\r\n",
        "\r\n",
        "args = ['--data-folder', mnist_file_dataset.as_mount(), '--regularization', 0.5]\r\n",
        "\r\n",
        "src = ScriptRunConfig(source_directory=script_folder,\r\n",
        "                      script='train.py', \r\n",
        "                      arguments=args,\r\n",
        "                      compute_target=compute_target,\r\n",
        "                      environment=env)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we are finally running our expiriment with the configs we set in the cell above.\r\n",
        "\r\n",
        "run = exp.submit(config=src)\r\n",
        "run"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#With this code, we can track the progress of our program running. \r\n",
        "\r\n",
        "from azureml.widgets import RunDetails\r\n",
        "RunDetails(run).show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We are waiting for completion before moving on to any other code. \r\n",
        "#We will be able to see the results after the program is complete.\r\n",
        "\r\n",
        "# specify show_output to True for a verbose log\r\n",
        "run.wait_for_completion(show_output=True) "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We are printing all the metrics recorded during the run. This includes things like accuracy.\r\n",
        "\r\n",
        "print(run.get_metrics())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#In order to deploy the model we have just created or allow other contributors to query or examine it, we need to register the model.\r\n",
        "\r\n",
        "# register model \r\n",
        "model = run.register_model(model_name='sklearn_mnist', model_path='outputs/sklearn_mnist_model.pkl')\r\n",
        "print(model.name, model.id, model.version, sep='\\t')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}